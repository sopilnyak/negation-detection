{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# conda install -y gensim\n",
    "# run cell\n",
    "# conda uninstall -y boto\n",
    "# conda install -y boto\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from gensim.models.wrappers import FastText\n",
    "import pandas as pd\n",
    "import pymorphy2\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import _pickle as cPickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Настройки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Путь к папке с csv-файлами корпуса\n",
    "corpora_path = './data/2017.12.09-1959-Polarity-Job1943-ru-RU/Reuters_russian_articles/'\n",
    "\n",
    "# Путь к модели эмбеддингов\n",
    "# model_path = './models/news.model.bin'  # маленький корпус\n",
    "# model_path = '../models/ruwikiruscorpora.model.bin'  # большой корпус\n",
    "model_path = 'rutenten11_8.bin'  # большой корпус со стоп-словами\n",
    "\n",
    "results_path = 'rutent3'  # директория, куда сохранять результаты\n",
    "\n",
    "# Количество файлов\n",
    "FILE_NUMBER = 18000\n",
    "\n",
    "EMBEDDING_SIZE = 100  # размер эмбеддинга\n",
    "MAX_SEQ_LENGTH = 30  # максимальное количество слов в предложении\n",
    "N_BATCH = 128  # размер батча\n",
    "\n",
    "TRAIN_SPLIT = 0.7  # доля обучающей выборки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Берем все csv-файлы из нужной директории\n",
    "directory = glob.glob(os.path.join(corpora_path, '*.csv'))\n",
    "assert(len(directory) > 0)\n",
    "len(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# embeddings_vectors = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "embeddings_vectors = FastText.load_fasttext_format(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# # Преобразуем объект EuclidianKeyedVectors в dict (для второго корпуса)\n",
    "\n",
    "# embeddings_dict = {}\n",
    "# for key in embeddings_vectors.vocab:\n",
    "#     raw_word = key.split(\"_\")[0]\n",
    "#     embeddings_dict[raw_word] = embeddings_vectors[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_dict = embeddings_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Размер эмбеддинга\n",
    "EMBEDDING_SIZE = len(embeddings_dict['не'])\n",
    "\n",
    "# Вектора, обозначающие является ли слово cue или нет\n",
    "CUE_VECTOR = np.ones(EMBEDDING_SIZE)\n",
    "NOT_CUE_VECTOR = np.zeros(EMBEDDING_SIZE)\n",
    "\n",
    "# Пустой вектор\n",
    "ZERO_VECTOR = np.zeros(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Количество предложений (только для теста)\n",
    "sentenses_count = 0\n",
    "\n",
    "# Слова, которых нет в словаре эмбеддингов (только для теста)\n",
    "unsuccessful_embeddings = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_word(word):\n",
    "    \n",
    "    word = word.lower()\n",
    "\n",
    "    # Если слово содержит цифры или другие посторонние символы, выкинем его\n",
    "    pattern = re.compile(\"[^а-яА-Яё]\")\n",
    "    if pattern.match(word) != None:\n",
    "        return False\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Для предложения data создает матрицу эмбеддингов и меток, где номер строки отвечает номеру слова.\n",
    "'''\n",
    "\n",
    "def generate_batch(data):\n",
    "\n",
    "    # Матрица эмбеддингов, номер строки -- это номер слова\n",
    "    embeddings = []\n",
    "\n",
    "    # Cue-матрица, номер строки -- это номер слова\n",
    "    cues = []\n",
    "    \n",
    "    # Вектор ответов для каждого слова\n",
    "    targets = []\n",
    "    \n",
    "    # Текст предложения\n",
    "    text = \"\"\n",
    "    \n",
    "    # Сколько слов получилось по факту (после выкидывания неудавшихся эмбеддингов)\n",
    "    n_words = 0\n",
    "\n",
    "    for row_number in range(len(data)):\n",
    "\n",
    "        # Для каждого слова получаем эмбеддинг и записываем в матрицу\n",
    "        if (not pd.isnull(data['Text'][row_number])):\n",
    "\n",
    "            word_orig = data['Text'][row_number].split(\"=\")[1]\n",
    "            word = prepare_word(word_orig)\n",
    "            if (not word):\n",
    "                continue\n",
    "\n",
    "            # Пытаемся получить эмбеддинг слова\n",
    "            try:\n",
    "                # Записываем в матрицу эмбеддингов\n",
    "                embeddings.append(embeddings_dict[word])\n",
    "                #embeddings = np.append(embeddings, [embeddings_dict[word]], axis=0)\n",
    "                \n",
    "                if (word == 'не' or word == 'нет'):\n",
    "                    cue_vector = CUE_VECTOR\n",
    "                else:\n",
    "                    cue_vector = NOT_CUE_VECTOR\n",
    "                    \n",
    "                # Записываем в матрицу cue\n",
    "                cues.append(cue_vector)\n",
    "\n",
    "                if (pd.isnull(data['Tags'][row_number])):\n",
    "                    target = [0, 1]\n",
    "                else:\n",
    "                    target = [1, 0]\n",
    "\n",
    "                # Записываем в вектор ответов\n",
    "                targets.append(target)\n",
    "                \n",
    "                text += word_orig + \" | \"\n",
    "                n_words += 1\n",
    "\n",
    "            except KeyError:\n",
    "                # В случае неудачи сохраняем в список неудавшихся слов\n",
    "                unsuccessful_embeddings.append(word)\n",
    "                \n",
    "    # Padding\n",
    "    for padding_index in range(MAX_SEQ_LENGTH - n_words):\n",
    "        embeddings.append(ZERO_VECTOR)\n",
    "        cues.append(ZERO_VECTOR)\n",
    "        targets.append([0, 0])\n",
    "    \n",
    "    # Ограничиваем длину предложения величиной MAX_SEQ_LENGTH\n",
    "    embeddings = np.array(embeddings[:MAX_SEQ_LENGTH])\n",
    "    cues = np.array(cues[:MAX_SEQ_LENGTH])\n",
    "    targets = np.array(targets[:MAX_SEQ_LENGTH])\n",
    "    \n",
    "    return np.concatenate((embeddings, cues), axis=1), targets, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Генерация массива данных размерности (количество предложений, максимальная длина предложения, 2 * размер эмбеддинга)\n",
    "Вектора эмбеддингов и меток конкатенируются в один вектор длины 2 * EMBEDDING_SIZE\n",
    "\n",
    "targets: (количество предложений, максимальная длина предложения, 2)\n",
    "[0, 1] - слово не принадлежит классу отрицания\n",
    "[1, 0] - слово принадлежит классу отрицания\n",
    "'''\n",
    "\n",
    "# Матрица сконкатенированных embeddings и cues\n",
    "embedding_batch = []\n",
    "\n",
    "# Матрица ответов\n",
    "target_batch = []\n",
    "\n",
    "# Список текстов предложений\n",
    "texts = []\n",
    "\n",
    "# Проходимся по всем файлам\n",
    "for filename in tqdm(directory[:FILE_NUMBER]):\n",
    "\n",
    "    # Считаем из файла в Pandas Dataframe\n",
    "    file = pd.read_csv(filename, sep='\\t', names=['Offset', 'Text', 'Tags'], skip_blank_lines=False)\n",
    "    \n",
    "    sentence = pd.DataFrame()\n",
    "    words_in_sentence_count = 0\n",
    "    \n",
    "    for row_number in range(len(file)):\n",
    "\n",
    "        # Предложения разделены пустой строкой, поэтому чтобы посчитать количество \n",
    "        # предложений, посчитаем количество пустых строк и потом прибавим 1\n",
    "        if (words_in_sentence_count > MAX_SEQ_LENGTH or pd.isnull(file['Offset'][row_number])):\n",
    "            \n",
    "            words_in_sentence_count = 0\n",
    "            is_new_sentense = True\n",
    "            \n",
    "            # Добавляем получившиеся эмбеддинги предложения в общий список\n",
    "            sent_embeddings, sent_targets, sent_text = generate_batch(sentence)\n",
    "            \n",
    "            assert(sent_embeddings.shape == (MAX_SEQ_LENGTH, 2 * EMBEDDING_SIZE))\n",
    "            assert(sent_targets.shape == (MAX_SEQ_LENGTH, 2))\n",
    "            \n",
    "            embedding_batch.append(sent_embeddings)\n",
    "            target_batch.append(sent_targets)\n",
    "            \n",
    "            texts.append(sent_text)\n",
    "            \n",
    "            # Стартуем новое предложение\n",
    "            sentence = pd.DataFrame()\n",
    "            \n",
    "        else:\n",
    "            sentence = sentence.append(file.iloc[[row_number]], ignore_index=True)\n",
    "            words_in_sentence_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_batch = np.array(embedding_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_batch = np.array(target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array(embedding_batch).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Слова, для которых не удалось получить эмбеддинги\n",
    "set(unsuccessful_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_batch = len(embedding_batch)\n",
    "n_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = embedding_batch[:int(n_batch * TRAIN_SPLIT)]\n",
    "X_test = embedding_batch[int(n_batch * TRAIN_SPLIT):]\n",
    "y = target_batch[:int(n_batch * TRAIN_SPLIT)]\n",
    "y_test = target_batch[int(n_batch * TRAIN_SPLIT):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"X shape:\", X.shape)\n",
    "print(\"y shape:\", y.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "print(\"y_test shape:\", y_test.shape)\n",
    "print(\"texts length:\", len(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сохраним посчитанные матрицы в файлы\n",
    "\n",
    "cPickle.dump(X, open(os.path.join(results_path, \"X_train.pkl\"), \"wb\"))\n",
    "cPickle.dump(X[:10000], open(os.path.join(results_path, \"X_train1.pkl\"), \"wb\"))\n",
    "cPickle.dump(X[10000:20000], open(os.path.join(results_path, \"X_train2.pkl\"), \"wb\"))\n",
    "cPickle.dump(X[20000:], open(os.path.join(results_path, \"X_train3.pkl\"), \"wb\"))\n",
    "cPickle.dump(y, open(os.path.join(results_path, \"y_train.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cPickle.dump(X_test, open(\"X_test.pkl\", \"wb\"))\n",
    "# cPickle.dump(y_test, open(\"y_test.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cPickle.dump(texts, open(os.path.join(results_path, \"texts.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
